<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title>Amitrajit Sarkar</title>
		<link href="style.css" rel="stylesheet" type="text/css">
		<link href='https://fonts.googleapis.com/css?family=Lora' rel='stylesheet' type='text/css'>
		<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	</head>

	<body id="body">
		<div id="namesection" class="navigator">
			<h1 class="centered c0">AMITRAJIT SARKAR</h1>
			<h1 class="centered c2">ME, MYSELF, I.</h1>
			<img id="mute" class="icon" src="images/sound.png">

			<p class="centered c0">
				<img class="icon" src="images/gmail.png">
				<a class="link" href="mailto:aaiijmrtt@gmail.com">Gmail</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/github.png">
				<a class="link" href="https://github.com/aaiijmrtt">Github</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/twitter.png">
				<a class="link" href="https://twitter.com/aaiijmrtt">Twitter</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/facebook.png">
				<a class="link" href="https://www.facebook.com/aaiijmrtt">Facebook</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/yourehere.png">
				<a class="link" href="http://aaiijmrtt.github.io">You're Here</a>
			</p>
		</div>

		<div id="subnavigation" class="navigator">
			<h1 class="centered c1 c2">
				<span class="c11">PERSONALLY</span>
				<span class="c12">PROFESSIONALLY</span>
				<span class="c211">MUSICALLY</span>
				<span class="c212">POETICALLY</span>
				<span class="c213">ARTISTICALLY</span>
				<span class="c214">IMAGINATIVELY</span>
				<span class="c215">MATHEMATICALLY</span>
				<span class="c216">DIGITALLY</span>
				<span class="c221">NEURAL NETWORKS</span>
				<span class="c222">GRADIENT DESCENT</span>
				<span class="c223">WORD VECTORS</span>
			</h1>

			<span id="leftbutton" class="c2 link">THIS WAY</span>
			<span id="rightbutton" class="c2 link">THAT WAY</span>

			<p id="l10" class="centered c11">
					<p id="l11" class="centered c11 link">MUSICALLY</p>
					<p id="l12" class="centered c11 link">POETICALLY</p>
					<p id="l13" class="centered c11 link">ARTISTICALLY</p>
					<p id="l14" class="centered c11 link">IMAGINATIVELY</p>
					<p id="l15" class="centered c11 link">MATHEMATICALLY</p>
					<p id="l16" class="centered c11 link">DIGITALLY</p>
			</p>

			<p id="l20" class="centered c12">
				<p id="l21" class="centered c12 link">NEURAL NETWORKS</p>
				<p id="l22" class="centered c12 link">GRADIENT DESCENT</p>
				<p id="l23" class="centered c12 link">WORD VECTORS</p>
			</p>
		</div>

		<div id="supernavigation" class="navigator">
			<span id="upbutton" class="c2 link">UP, UP, AWAY</span>

			<h1 class="centered c0 c2">
				<span class="c0">ABOUT ME</span>
				<span class="c2">MORE ABOUT ME</span>
			</h1>

			<p id="l0" class="centered c0">
				<p id="l1" class="centered c0 link">PERSONALLY</p>
				<p id="l2" class="centered c0 link">PROFESSIONALLY</p>
			</p>
		</div>

		<div id="imagesection" class="presenter">
			<img class="image centered c211" src="images/musically.jpg">
			<img class="image centered c212" src="images/poetically.jpg">
			<img class="image centered c213" src="images/artistically.jpg">
			<img class="image centered c214" src="images/imaginatively.jpg">
			<img class="image centered c215" src="images/mathematically.jpg">
			<img class="image centered c216" src="images/digitally.jpg">

			<div class="c221">
				<p>
As far as I know, neural networks are becoming pretty good at a lot of things these days. What are they? In essence, we may think of a neural network as a
function $f$ controlled by the set of parameters $\theta$ between hyperspaces $\Re^p$ and $\Re^q$, ie. $f_\theta : \Re^p \to \Re^q$. But first, let us describe
a single neuron, intuitively, following its most common mathematical formulation.
				</p>
				<p>
Biologically, a neuron receives impulses from other neurons, is excited by them to an extent, and subsequently generates a response. If a neuron receives
impulses from $p$ other neurons, its impulse $x$ to response $z$ characteristics may be mathematically modeled as a map $n_w: \Re^p \to \Re$. Some neurons
excite it more, some less: hence its excitement may be approximated as a weighted mean, ie. $\bar{a} = \Sigma^{p}_{i = 1} w_i \cdot x_i$. The neuron is usually
sensitive to a particular range of excitements and its response generally trails off with 'distance' from this range. We may fix the center of this range by
adding a bias to the excitement, ie. $a = \Sigma^{p}_{i = 1} w_i \cdot x_i + b$. The response may then be considered a nonlinear function of the excitement,
ie. $z = \frac{1}{1 + e^{-a}}$. This function $z(a)$ is called the sigmoid.
				</p>
				<p>
Now if we stack $q$ of these neurons into a layer, each with their separate set of weights $w \in W$, we obtain a neural layer $l_W: \Re^p \to \Re^q$. And if
we connect layers by feeding the outputs of one layer to the inputs of the next, we obtain a neural network. It really is that simple. But is this artificial
construction enough for them to do all the neat things we have heard of? Not nearly. Then, how do they do it? They aren't 'special': they too must be trained.
There are basically $2$ types of training: supervised and unsupervised. By extension there are $2$ classes of networks, depending on how they have been
trained. Supervised training is generally used to make function approximators, while unsupervised training is used to make feature extractors.
				</p>
				<p>
A supervised training set consists of training samples of inputs and expected outputs, and looks like $\{(x_i, y_i) | x_i \in \Re^p, y_i \in \Re^q\}_{i =
1}^{n}$. The goal is to have the neural network learn a generalized mapping such that $f_\theta (x_i) \approx y_i \forall i$. We choose a set of parameters
$\theta^\star$ which minimizes a cost function $||: \Re^q \times \Re^q \to \Re$ as $\theta^\star = \underset{\theta}{\text{argmin}} \Sigma^{n}_{i = 1}|
f_\theta(x_i), y_i|$. That does not seem too difficult. But the beauty of neural networks is that they can even generate 'sensible' outputs even for inputs
which were not in the training set: they can generalize, they can 'learn'.
				</p>
				<p>
An unsupervised training set, on the other hand, consists only of inputs, and looks like $\{x_i | x_i \in \Re^p\}_{i = 1}^{n}$. The goal is far more involved,
and the outcome far more intriguing. The network learns a q dimensional representation of the p dimensional data, which can be used in place of the original
vector, and thought of as a tuple of extracted features. Why is this so brilliant? Because it allows us to construct hierarchical abstractions for different
forms of data representation, giving rise to some very exciting properties. How is this done? A common method is to define a $2^{nd}$ reconstruction function
$g_\phi : \Re^q \to \Re^p$ to apporximate $g_\phi(f_\theta(x_i)) \approx x_i \forall i$.
				</p>
				<p>
Once trained, what can a network do? In general, it is expected to do $1$ of $2$ things: classification or regression. By extension, there are $2$ classes of
neural networks depending on what they do. While classifying, the network attempts to label input vectors into $1$ or more of $q$ discrete classes and hence
its range looks like $\{0, 1\}^q$ in binary or $\{-1, 1\}^q$ in bipolar vectors. While regressing, the network attempts to produce continuous intermediate
values for all input vectors and hence its range looks like $\Re^q$.
				</p>
			</div>

			<div class="c222">
				<p>
As far as I know, neural networks must be systematically trained before they can do anything at all. How? The secret behind their success is gradient descent.
The simplest explanation of gradient descent is that if we were trying to change the parameters to minimize the cost function, we would want to move the
parameters by a small amount in the direction in which the cost decreases the most. If the cost function was differentiable with respect to the parameters,
we would like to iteratively do $\theta^{t + 1} \leftarrow \theta^t - \alpha \cdot \nabla^t_{\theta^t}|f_{\theta^t}(x), y|$ with time t. If we imagine
ourselves as a parameter configuration, trying to find our way downhill across the parameter space, how would we go about it? Our best bet would perhaps be to
follow the steepest slope downwards.
				</p>
				<p>
But how do we know that these small steps will actually lead somewhere? And if they do, how can we tell that we have found a good spot, and are not simply
stuck in a ditch, the kind they call a local minima? We don't. We can't. We are exploring an unknown land, and have very little idea what we may find. However,
we may prepare ourselves to follow traditional knowledge passed down through generations by fellow explorers. We shall discuss some of that wisdom, on how to
measure our steps with time, over and above simply following the lay of the land, to chart a course for ourselves. We represent $\nabla^t_{\theta^t}
|f_{\theta^t}(x), y|$ by $\delta^t$.
				</p>
				<p>
If we take long steps, would we jump across valleys we would otherwise have liked to descend into? If we take short steps, would we remain stuck in a wayside
ditch we could otherwise have climbed out of? Perhaps. Perhaps not. The best we can do is to guess: when we started on our quest, we were probably a long way
off from our target and hence should have taken long steps, shortening them as we near our goal so as not to overshoot. This spirit is captured by the steps
$\frac{\alpha}{\beta^t} \cdot \delta^t$, where $\beta^t = 1 + \tau \cdot t$ for decay, $(\beta^t)^2 = (\beta^{t - 1})^2 + (\delta^t)^2$ for adaptive gradients,
$(\beta^t)^2 = \mu(\beta^{t - 1})^2 + (1 - \mu) \cdot (\delta^t)^2 $ for root mean square propagation.
				</p>
				<p>
If we took a step in a direction that looked good, would our confidence grow and would we be encouraged to take longer steps? Similarly, if we took a step in
the direction that didn't look promising, wouldn't our confidence shrink? This idea is captured by the steps $\alpha \cdot \gamma^t \odot \delta^t$, where
$\gamma^t_i = \begin{cases} \gamma^{t - 1}_i + \Delta, & \delta^t_{i} \cdot \delta^{t - 1}_{i} > 0\\ \gamma^{t - 1}_i \cdot (1 - \Delta), & \delta^t_{i} \cdot
\delta^{t - 1}_{i} < 0\\ \gamma^{t - 1}_i, & otherwise \\ \end{cases}$ for adaptive gain, $\gamma^t_i = \begin{cases} \gamma^{t - 1}_i \cdot (1 + \Delta), &
\delta^t_{i} \cdot \delta^{t - 1}_{i} > 0\\ \gamma^{t - 1}_i \cdot (1 - \Delta), & \delta^t_{i} \cdot \delta^{t - 1}_{i} < 0\\ \gamma^{t - 1}_i, & otherwise\\
\end{cases}$ for bar delta bar. Resilient propagation goes as far as to take steps solely based on confidence, without worrying about how steep the terrain is:
it steps as $\alpha \cdot \gamma^t \odot sgn(\delta^t)$.
				</p>
				<p>
If we had been travelling in one direction for a while, would we develop a fondness for that direction, and ignore minor permonitory bumps along the way? This
thought is captured by the concept of momentum, which replaces the steps by a speed, $\theta^{t + 1} \leftarrow \theta^t - v^t$ where $v^t = \gamma \cdot v^{t
- 1} + \alpha \cdot \nabla^t_{\theta^t}|f_{\theta^t}(x), y|$.
				</p>
				<p>
What about the terrain in the first place? Where did it come from? Can it be sculpted or moulded? The answer to that lies in the nature of the network, the
cost function, and their interaction with the dataset. A few of the common cost functions $|f_{\theta}(x), y|$ are the mean squared $(f_{\theta}(x)  - y) \cdot
(f_{\theta}(x)  - y)$, the cross entropy $-(y \cdot \ln(f_{\theta}(x)) + (1 - y) \cdot \ln(1 - f_{\theta}(x)))$, the negative log likelihood $-y \cdot \ln(
f_{\theta}(x))$, the divergence $y \cdot (\ln(y) - \ln(f_{\theta}(x)))$ and the cosine distance $\frac{f_{\theta}(x)}{|f_{\theta}(x)|} \cdot \frac{y}{|y|}$.
				</p>
				<p>
If we choose to meticulously measure our slopes before every step, taking the opinion of each and every training sample into account, we would be performing
batch gradient descent on the cost $\frac{1}{n} \Sigma^{n}_{i = 1}|f_\theta(x_i), y_i|$. If we choose to only listen to a single training sample, we would be
performing stochastic gradient descent on the cost $|f_\theta(x_i), y_i|$. Anything between the two extremes uses minibatches.
				</p>
			</div>

			<div class="c223">
				<p>
As far as I know, neural networks cannot inherently understand humanspeak. Words need to be represented in a form that computers can interpret. The challenge
lies in that fact that alphabetically speaking, words which look starkly dissimilar may be identical in meaning, while words which look alike may mean
altogether different things. What makes things worse is that the meaning of words may even vary with context. Computers, being logically deterministic, have
trouble interpreting these nuances. Rules to capture them fail to generalize, and show very little similarity across languages, making the work extremely
tedious. Thankfully, computers are good with numbers. So we arrive at the common ground: word vectors.
				</p>
				<p>
The simplest representation is to use a one-hot encoding. Thus, each word $w$, in an word vocabulary $V$, has a vectorial representation, $v \in\{0, 1\}^{|V|}$
constrained such that $|v| = 1$ and for two words, $w = w\prime \iff v = v\prime$. We often refer to the word having $v_i = 1$ as the $i^{th}$ word in the
vocabulary, $v^{(i)}$. But for any language with a decent sized vocabulary, $|V|$ is rather large. To make matters worse, this encoding captures no concept of
the similarity between words whatsoever. All Hamming Distances are $2$, all Cosine Distances are $0$, among other bad things that can be said about it. So
where do we go from there?
				</p>
				<p>
Deeper, in terms of neural network layers. Denser, in terms of the size of the vector space. Continuous, in terms of the field under the vector space. Until
the representations begin to look more like $v \in \Re^m$ with $m \ll |V|$. That being the destination, but how do we get there? We would like to ensure that
these have some 'sense' embedded within them. We adopt the policy of recognizing a word by the company it usually keeps. We can divide models for them in
broadly two categories: the continuous bag of words and the skip gram models. The good news is that these models can be trained over large corpuses, completely
unsupervised. A peculiarity of the neural networks used to generate both these models is that the hidden, intermediate layer does not contain a nonlinear
activation function. All $3$ layers are fully connected, but strictly linear. These models give rise to some neat properties of their word vectors, too. For
example, once you are done training, you might discover that $v_{queen} \approx v_{king} - v_{man} + v_{woman}$. Indeed, it is a matter of no mere coincidence,
for such additive patterns generalize to several words.
				</p>
				<p>
The intuition between the continuous bag of words model is to use the context in which a word is found to predict a probability distribution of itself. So we
construct a neural network where the input layer is the mean of the one-hot encodings of words in the context, the intermediate layer given the dense neural
word embedding, and the outer layer is softmaxed to generate the probability distribution of the word itself. This output is optimized with respect to a cost
function, generally the negative log likelihood, against the one-hot encoding of the centre word itself.
				</p>
				<p>
The intuition between the skip gram model is to use the centre word to predict a probability distribution over the context in which it is found. Though the
neural network construction is nearly identical, its input and output are in a sense reversed. Instead of taken the mean of the context in the input layer and
optimizing the cost function against the one-hot encoding of the centre word, we take the one-hot encoding of the centre word in the input layer and optimize
the cost function against the mean of the context.
				</p>
				<p>
How do we capture these intuitions mathematically? The number of words, $C$, included in the context on either side of the centre word is called the size of
the context window. Say the centre word is $w^{(i)}$, and the context words are $w^{(j)}, \forall j \in \mathbb{Z} | 0 < |j| <= C$. If $W_I$ and $W_O$ be the
input and output layer weights, respectively, and $v_I \in W_I$ and $v_O \in W_O$ be a column and row in these matrices, then the optimization of the continuous
bag of words model looks like $\underset{W_{I}, W_{O}}{\text{min}} -\log\frac{\exp{({v_O^{(i)}}^T \frac{1}{2C} \displaystyle\sum_{\substack{j = -C \\ j \neq 0}
}^{C} v_I^{(j)})}}{\displaystyle\sum_{k = 1}^{|V|}\exp({{v_O^{(i)}}^T v_I^{(k)}})}$, while that for the skip gram model looks like $\underset{W_{I}, W_{O}}{
\text{min}} -\frac{1}{2C}\displaystyle\sum_{\substack{j = -C \\ j \neq 0}}^{C}\log\frac{\exp{({v_O^{(j)}}^T v_I^{(i)})}}{\displaystyle\sum_{k = 1}^{|V|}\exp({{
v_O^{(k)}}^T v_I^{(i)}})}$
				</p>
			</div>
		</div>

		<div id="textsection" class="presenter">
			<h1 class="centered c211">Melodies</h1>
			<p class="centered c211">I used to play the piano, once upon a time.</p>
			<p class="centered c211">We lost touch, a while back.</p>
			<p class="centered c211">Now she refuses to play me.</p>

			<h1 class="centered c212">Verses</h1>
			<p class="centered c212">My muse.</p>
			<p class="centered c212">My ruse.</p>
			<p class="centered c212">My mistress.</p>

			<h1 class="centered c213">Colours</h1>
			<p class="centered c213">I was always blind to that beauty of your world.</p>
			<p class="centered c213">That is why it is so dark in here.</p>

			<h1 class="centered c214">Dreams</h1>
			<p class="centered c214">Sometimes I stop in wonder, and wander.</p>
			<p class="centered c214">To fill the gaps the world left me.</p>

			<h1 class="centered c215">Symbols</h1>
			<p class="centered c215">I loved her, first.</p>
			<p class="centered c215">I'm still waiting for her to love me back.</p>

			<h1 class="centered c216">Logic</h1>
			<p class="centered c216">This is how people know me.</p>
			<p class="centered c216">They never paid enough attention anyway.</p>

			<h1 class="centered c221">Neural Networks</h1>
			<p class="centered c221">How computers seem to 'think' these days.</p>

			<h1 class="centered c222">Gradient Descent</h1>
			<p class="centered c222">How computers seem to 'learn' these days.</p>

			<h1 class="centered c223">Word Vectors</h1>
			<p class="centered c223">How computers seem to 'read' these days.</p>
		</div>

		<div id="backsection">
			<img id="back" class="centered" src="images/background1.jpeg">
			<audio id="music" autoplay="autoplay" loop="true">
				<source src="music.ogg" type="audio/mp3">
			</audio>
		</div>

		<script src="script.js"></script>
	</body>
</html>
