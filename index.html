<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title>Amitrajit Sarkar</title>
		<link href="style.css" rel="stylesheet" type="text/css">
		<link href='http://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet' type='text/css'>
		<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	</head>

	<body id="body">
		<div id="namesection" class="navigator">
			<h1 class="centered c0">AMITRAJIT SARKAR</h1>
			<h1 class="centered c2">ME, MYSELF, I.</h1>
			<img id="mute" class="icon" src="images/sound.png">

			<p class="centered c0">
				<img class="icon" src="images/gmail.png">
				<a class="link" href="mailto:aaiijmrtt@gmail.com">Gmail</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/github.png">
				<a class="link" href="https://github.com/aaiijmrtt">Github</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/twitter.png">
				<a class="link" href="https://twitter.com/aaiijmrtt">Twitter</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/facebook.png">
				<a class="link" href="https://www.facebook.com/aaiijmrtt">Facebook</a>
			</p>

			<p class="centered c0">
				<img class="icon" src="images/yourehere.png">
				<a class="link" href="http://aaiijmrtt.github.io">You're Here</a>
			</p>
		</div>

		<div id="subnavigation" class="navigator">
			<h1 class="centered c1 c2">
				<span class="c11">PERSONALLY</span>
				<span class="c12">PROFESSIONALLY</span>
				<span class="c211">MUSICALLY</span>
				<span class="c212">POETICALLY</span>
				<span class="c213">ARTISTICALLY</span>
				<span class="c214">IMAGINATIVELY</span>
				<span class="c215">MATHEMATICALLY</span>
				<span class="c216">DIGITALLY</span>
				<span class="c221">NEURAL NETWORKS</span>
				<span class="c222">GRADIENT DESCENT</span>
			</h1>

			<span id="leftbutton" class="c2 link">THIS WAY</span>
			<span id="rightbutton" class="c2 link">THAT WAY</span>

			<p id="l10" class="centered c11">
					<p id="l11" class="centered c11 link">MUSICALLY</p>
					<p id="l12" class="centered c11 link">POETICALLY</p>
					<p id="l13" class="centered c11 link">ARTISTICALLY</p>
					<p id="l14" class="centered c11 link">IMAGINATIVELY</p>
					<p id="l15" class="centered c11 link">MATHEMATICALLY</p>
					<p id="l16" class="centered c11 link">DIGITALLY</p>
			</p>

			<p id="l20" class="centered c12">
				<p id="l21" class="centered c12 link">NEURAL NETWORKS</p>
				<p id="l22" class="centered c12 link">GRADIENT DESCENT</p>
			</p>
		</div>

		<div id="supernavigation" class="navigator">
			<span id="upbutton" class="c2 link">UP, UP, AWAY</span>

			<h1 class="centered c0 c2">
				<span class="c0">ABOUT ME</span>
				<span class="c2">MORE ABOUT ME</span>
			</h1>

			<p id="l0" class="centered c0">
				<p id="l1" class="centered c0 link">PERSONALLY</p>
				<p id="l2" class="centered c0 link">PROFESSIONALLY</p>
			</p>
		</div>

		<div id="imagesection" class="presenter">
			<img class="image centered c211" src="images/musically.jpg">
			<img class="image centered c212" src="images/poetically.jpg">
			<img class="image centered c213" src="images/artistically.jpg">
			<img class="image centered c214" src="images/imaginatively.jpg">
			<img class="image centered c215" src="images/mathematically.jpg">
			<img class="image centered c216" src="images/digitally.jpg">

			<div class="c221">
				<p>
As far as I know, neural networks are becoming pretty good at a lot of things these days. What are they? In essence, we may think of a neural network as a
function $f$ controlled by the set of parameters $\theta$ between hyperspaces $\Re^p$ and $\Re^q$, ie. $f_\theta : \Re^p \to \Re^q$. But first, let us describe
a single neuron, intuitively, following its most common mathematical formulation.
				</p>
				<p>
Biologically, a neuron receives impulses from other neurons, is excited by them to an extent, and subsequently generates a response. If a neuron receives
impulses from $p$ other neurons, its impulse $x$ to response $z$ characteristics may be mathematically modeled as a map $n_w: \Re^p \to \Re$. Some neurons
excite it more, some less: hence its excitement may be approximated as a weighted mean, ie. $\bar{a} = \Sigma^{p}_{i = 1} w_i \cdot x_i$. The neuron is usually
sensitive to a particular range of excitements and its response generally trails off with 'distance' from this range. We may fix the center of this range by
adding a bias to the excitement, ie. $a = \Sigma^{p}_{i = 1} w_i \cdot x_i + b$. The response may then be considered a nonlinear function of the excitement,
ie. $z = \frac{1}{1 + e^{-a}}$. This function $z(a)$ is called the sigmoid.
				</p>
				<p>
Now if we stack $q$ of these neurons into a layer, each with their separate set of weights $w \in W$, we obtain a neural layer $l_W: \Re^p \to \Re^q$. And if
we connect layers by feeding the outputs of one layer to the inputs of the next, we obtain a neural network. It really is that simple. But is this artificial
construction enough for them to do all the neat things we have heard of? Not nearly. Then, how do they do it? They aren't 'special': they too must be trained.
There are basically $2$ types of training: supervised and unsupervised. By extension there are $2$ classes of networks, depending on how they have been
trained. Supervised training is generally used to make function approximators, while unsupervised training is used to make feature extractors.
				</p>
				<p>
A supervised training set consists of training samples of inputs and expected outputs, and looks like $\{(x_i, y_i) | x_i \in \Re^p, y_i \in \Re^q\}_{i =
1}^{n}$. The goal is to have the neural network learn a generalized mapping such that $f_\theta (x_i) \approx y_i \forall i$. We choose a set of parameters
$\theta^\star$ which minimizes a cost function $||: \Re^q \times \Re^q \to \Re$ as $\theta^\star = \underset{\theta}{\text{argmin}} \Sigma^{n}_{i = 1}|
f_\theta(x_i), y_i|$. That does not seem too difficult. But the beauty of neural networks is that they can even generate 'sensible' outputs even for inputs
which were not in the training set: they can generalize, they can 'learn'.
				</p>
				<p>
An unsupervised training set, on the other hand, consists only of inputs, and looks like $\{x_i | x_i \in \Re^p\}_{i = 1}^{n}$. The goal is far more involved,
and the outcome far more intriguing. The network learns a q dimensional representation of the p dimensional data, which can be used in place of the original
vector, and thought of as a tuple of extracted features. Why is this so brilliant? Because it allows us to construct hierarchical abstractions for different
forms of data representation, giving rise to some very exciting properties. How is this done? A common method is to define a $2^{nd}$ reconstruction function
$g_\phi : \Re^q \to \Re^p$ to apporximate $g_\phi(f_\theta(x_i)) \approx x_i \forall i$.
				</p>
				<p>
Once trained, what can a network do? In general, it is expected to do $1$ of $2$ things: classification or regression. By extension, there are $2$ classes of
neural networks depending on what they do. While classifying, the network attempts to label input vectors into $1$ or more of $q$ discrete classes and hence
its range looks like $\{0, 1\}^q$ in binary or $\{-1, 1\}^q$ in bipolar vectors. While regressing, the network attempts to produce continuous intermediate
values for all input vectors and hence its range looks like $\Re^q$.
				</p>
			</div>

			<div class="c222">
				<p>
As far as I know, neural networks must be systematically trained before they can do anything at all. How? The secret behind their success is gradient descent.
The simplest explanation of gradient descent is that if we were trying to change the parameters to minimize the cost function, we would want to move the
parameters by a small amount in the direction in which the cost decreases the most. If the cost function was differentiable with respect to the parameters,
we would like to iteratively do $\theta^{t + 1} \leftarrow \theta^t - \alpha \cdot \nabla^t_{\theta^t}|f_{\theta^t}(x), y|$ with time t. If we imagine
ourselves as a parameter configuration, trying to find our way downhill across the parameter space, how would we go about it? Our best bet would perhaps be to
follow the steepest slope downwards.
				</p>
				<p>
But how do we know that these small steps will actually lead somewhere? And if they do, how can we tell that we have found a good spot, and are not simply
stuck in a ditch, the kind they call a local minima? We don't. We can't. We are exploring an unknown land, and have very little idea what we may find. However,
we may prepare ourselves to follow traditional knowledge passed down through generations by fellow explorers. We shall discuss some of that wisdom, on how to
measure our steps with time, over and above simply following the lay of the land, to chart a course for ourselves. We represent $\nabla^t_{\theta^t}
|f_{\theta^t}(x), y|$ by $\delta^t$.
				</p>
				<p>
If we take long steps, would we jump across valleys we would otherwise have liked to descend into? If we take short steps, would we remain stuck in a wayside
ditch we could otherwise have climbed out of? Perhaps. Perhaps not. The best we can do is to guess: when we started on our quest, we were probably a long way
off from our target and hence should have taken long steps, shortening them as we near our goal so as not to overshoot. This spirit is captured by the steps
$\frac{\alpha}{\beta^t} \cdot \delta^t$, where $\beta^t = 1 + \tau \cdot t$ for decay, $(\beta^t)^2 = (\beta^{t - 1})^2 + (\delta^t)^2$ for adaptive gradients,
$(\beta^t)^2 = \mu(\beta^{t - 1})^2 + (1 - \mu) \cdot (\delta^t)^2 $ for root mean square gradient.
				</p>
				<p>
If we took a step in a direction that looked good, would our confidence grow and would we be encouraged to take longer steps? Similarly, if we took a step in
the direction that didn't look promising, wouldn't our confidence shrink? This idea is captured by the steps $\alpha \cdot \gamma^t \odot \delta^t$, where
$\gamma^t_i = \begin{cases} \gamma^{t - 1}_i + \Delta, & \delta^t_{i} \cdot \delta^{t - 1}_{i} > 0\\ \gamma^{t - 1}_i \cdot (1 - \Delta), & \delta^t_{i} \cdot
\delta^{t - 1}_{i} < 0\\ \gamma^{t - 1}_i, & otherwise \\ \end{cases}$ for adaptive gain, $\gamma^t_i = \begin{cases} \gamma^{t - 1}_i \cdot (1 + \Delta), &
\delta^t_{i} \cdot \delta^{t - 1}_{i} > 0\\ \gamma^{t - 1}_i \cdot (1 - \Delta), & \delta^t_{i} \cdot \delta^{t - 1}_{i} < 0\\ \gamma^{t - 1}_i, & otherwise\\
\end{cases}$ for bar delta bar. Resilient propagation goes as far as to take steps solely based on confidence, without worrying about how steep the terrain is:
it steps as $\alpha \cdot \gamma^t \odot sgn(\delta^t)$.
				</p>
				<p>
If we had been travelling in one direction for a while, would we develop a fondness for that direction, and ignore minor permonitory bumps along the way? This
thought is captured by the concept of momentum, which replaces the steps by a speed, $\theta^{t + 1} \leftarrow \theta^t - v^t$ where $v^t = \gamma \cdot v^{t
- 1} + \alpha \cdot \nabla^t_{\theta^t}|f_{\theta^t}(x), y|$.
				</p>
				<p>
What about the terrain in the first place? Where did it come from? Can it be sculpted or moulded? The answer to that lies in the nature of the network, the
cost function, and their interaction with the dataset. A few of the common cost functions $|f_{\theta}(x), y|$ are the mean squared $(f_{\theta}(x)  - y)^2$,
the cross entropy $-(y \odot \ln(f_{\theta}(x)) + (1 - y) \odot \ln(1 - f_{\theta}(x)))$, the negative log likelihood $-y \odot \ln(f_{\theta}(x))$, the
divergence $y \odot (\ln(y) - \ln(f_{\theta}(x)))$ and the cosine distance $\frac{f_{\theta}(x)}{|f_{\theta}(x)|} \cdot \frac{y}{|y|}$.
				</p>
				<p>
If we choose to meticulously measure our slopes before every step, taking the opinion of each and every training sample into account, we would be performing
batch gradient descent on the cost $\frac{1}{n} \Sigma^{n}_{i = 1}|f_{\theta^t}(x_i), y_i|$. If we choose to only listen to a single training sample, we would
be performing stochastic gradient descent on the cost $|f_{\theta^t}(x_i), y_i|$. Anything between the two extremes uses minibatches.
				</p>
			</div>
		</div>

		<div id="textsection" class="presenter">
			<h1 class="centered c211">Melodies</h1>
			<p class="centered c211">I used to play the piano, once upon a time.</p>
			<p class="centered c211">We lost touch, a while back.</p>
			<p class="centered c211">Now she refuses to play me.</p>

			<h1 class="centered c212">Verses</h1>
			<p class="centered c212">My muse.</p>
			<p class="centered c212">My ruse.</p>
			<p class="centered c212">My mistress.</p>

			<h1 class="centered c213">Colours</h1>
			<p class="centered c213">I was always blind to that beauty of your world.</p>
			<p class="centered c213">That is why it is so dark in here.</p>

			<h1 class="centered c214">Dreams</h1>
			<p class="centered c214">Sometimes I stop in wonder, and wander.</p>
			<p class="centered c214">To fill the gaps the world left me.</p>

			<h1 class="centered c215">Symbols</h1>
			<p class="centered c215">I loved her, first.</p>
			<p class="centered c215">I'm still waiting for her to love me back.</p>

			<h1 class="centered c216">Logic</h1>
			<p class="centered c216">This is how people know me.</p>
			<p class="centered c216">They never paid enough attention anyway.</p>

			<h1 class="centered c221">Neural Networks</h1>
			<p class="centered c221">How computers seem to 'think' these days.</p>

			<h1 class="centered c222">Gradient Descent</h1>
			<p class="centered c222">How computers seem to 'learn' these days.</p>
		</div>

		<div id="backsection">
			<img id="back" class="centered" src="images/background1.jpeg">
			<audio id="music" autoplay="autoplay" loop="true">
				<source src="music.ogg" type="audio/mp3">
			</audio>
		</div>

		<script src="script.js"></script>
	</body>
</html>
