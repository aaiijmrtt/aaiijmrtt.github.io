<p class="text">
As far as I know, humans and programs alike are capable of storing data. Neural networks should be able to do the same. One may argue that recurrent networks,
through their internal states, encode all the data they have been exposed to so far. However, the task of accurately storing and recalling long sequences
merely from a fixed length encoding is far from trivial. Neural networks should also be supplied with the ability to look up data from an auxiliary data
structure. Neural networks augmented with memory are called memory networks. The network is itself trained to read and write to its long-term storage, and may
use these memories at the time of inference.
</p>

<p class="text">
A memory network broadly consists of $4$ components: $\mathcal{I}, \mathcal{G}, \mathcal{O}, \mathcal{R}$. $\mathcal{I}$ maps an input, $x$, into an internal
feature representation, $\mathcal{I}(x)$. $\mathcal{G}$ generalizes the new input by assimilating it and updating its memories, $M_i = \mathcal{G}(M,
\mathcal{I}(x), M_i)$. $\mathcal{O}$ generates an output feature, $o = \mathcal{O}(M, \mathcal{I}(x))$, from its input and memory. $\mathcal{R}$ generates the
final response, $r = \mathcal{R}(o)$, from the output features. The exact nature and implementation of these components can be wide and varied.
</p>

<p class="text">
$\mathcal{I}$ may preprocess the text, or may simply project words into a feature space, transforming them into word vectors. $\mathcal{G}$, in its simplest
form, finds a slot in memory in which to put the input, often in the first available empty slot. The larger the capacity of memory, the more information the
network can store. However, more information slows down $\mathcal{O}$ and $\mathcal{R}$, and hence $\mathcal{G}$ may implement a method of updating relevant
existing memories, or forgetting irrelevant ones in a finite memory buffer. For even larger knowledge bases, memories may be grouped by category, allowing
recollections of only a smaller subset. $\mathcal{O}$ can generate a sequence of output feature vectors by referring to the most similar memories $o_n =
\mathcal{O}(\mathcal{I}(x), m) = \underset{i \in \mathbb{N}_{\leq |M|}}{\text{argmax}} s_{\mathcal{O}}((\mathcal{I}(x), (o_1 ... o_{n - 1})), m_i)$. Using
these, the response may be generated as a sentence using a recurrent neural network, or as a similar word from the vocabulary $V$, $r = \underset{w \in V}{
\text{argmax}} s_{\mathcal{R}}((\mathcal{I}(x), (o_1 ... o_n)), w)$. The number of supporting memories to be used, $n$, may be predetermined, or memories may
be generated until a designated end-of-sequence terminator is predicted. The nature of the similarity functions $s_{\mathcal{O}}$ and $s_{\mathcal{R}}$ may
vary.
</p>

<p class="text">
Another interesting, related concept is that of attention. It is especially common in the encoder-decoder models we discussed while exploring recurrences. The
decoder is allowed access to all the internal states of the encoder, $h_E^t$, which serves as a form of memory, $M = \begin{pmatrix} h_E^0 ... h_E^t ...
\end{pmatrix}$ during inference. However, to effectively utilize this additional information instead of becoming further confused, the network needs to focus
its attention on appropriately relevant portions of its memory.
</p>

<p class="text">
While hard attention models are not end to end differentiable, and gradient descent based optimization methods do not apply, soft attention models can be
trained end to end. During every decoding step, $t$, the memories are converted into a context vector, $c^t$, which is essentially a mean of the memories,
$c^t = M \cdot {a^t}^T$, weighted by an attention vector, $a^t$. The attention vector is obtained by a softmax, $S(x)_i = \frac{\exp{x_i}}{\sum_i\exp{x_i}}$,
of some similarity measure between the decoder hidden state, $h_D^t$, and the memories $M$. It may be a simple cosine similarity, $a^t = S({h_D^t}^T \cdot M)$,
or even a fairly complicated model such as an affine bilinear similarity measure, $a^t = S({h_D^t}^T \cdot W \cdot M + b)$.
</p>
