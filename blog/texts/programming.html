<p class="text">
As far as I know, humans have been programming computers for a while now. But can computers learn to program themselves? Inherently, writing a program involves
several forms of problem solving, requiring understanding and inspiration often not associated with the abilities of machines meant to manipulate bits. On the
other hand, what could be more natural to computers than programming? Deep Learning models are good at learning by example, so it is simply a matter of
substituting the lack of understanding and inspiration with diligent practice. They also have to be equipped with their own memory, reading and writing
mechanisms for accessing and modifying it, and the ability to use basic programming constructs such as selection, iterations, and function definition.
</p>

<p class="text">
Similar to Memory Networks, Neural Turing Machines have the ability to read from and write to an external memory. It is absolutely crucial that this mechanism
be differentiable. This is achieved by defining blurred read and write operations which interact to a greater or lesser degree with all the elements in memory.
The degree of blurriness is determined by an attention mechanism. A controller, which may be a feedforward or a recurrent neural network, receives external
inputs, interacts with the memory, and generates outputs.
</p>

<p class="text">
At any time $t$, weighting vectors, $w^t \in [0, 1]^N$ where $\sum^i w^t_i = 1$, control the attention of reading and writing heads on a memory bank $M^t \in
\Re^{N \times M}$. The vector $r^t = (M^t)^T \cdot w^t$, is read from the memory. How the memory is written is controlled by an erase vector, $e^t \in (0, 1)^M$
and an add vector, $a^t \in \Re^M$, following the rule $M^{t + 1} = M^t - M^t \odot w^t \cdot (e^t)^T + w^t \cdot (a^t)^T$. $w^t$ is generated by the controller
by combining a content based and a location based addressing scheme. Focusing by content, $w^t_c = \frac{\exp(\beta^t \cdot K(k^t, M^t))}{\sum_i \exp(\beta^t
\cdot K(k^t, M^t_i))}$, where the key strength, $\beta^t \in \Re_+$, is generated by the controller. The similarity kernel $K$, may be the cosine similarity.
Focusing by location, the vector is gated, shifted, sharpened and normalized. An interpolation gate, $g^t \in [0, 1]$, emitted by the controller controls the
gated weighting $w^t_g = g^t \cdot w^t_c + (1 - g^t) \cdot w^{t - 1}$. This is followed by a circular convolution $w^t_c = w^t_g * s^t$, controlled by the shift
weighting probability distribution $s^t$ generated by the controller. This is finally sharpened and normalized as $w^t = \frac{(w^t)^{\gamma^t}}{\sum_i
(w^t_i)^{\gamma^t}}$, where sharpening factor $\gamma^t \geq 1$ is also produced by the controller.
</p>

<p class="text">
For more complicated applications, machines must be able to represent and execute a clearly defined sequence of instructions sensibly. Neural Interpreter
Programmers accept a learnable program embedding, arguments for execution passed by the calling program, and a feature represenataion of the current state of
the programming environment, in order to generate the next instruction for execution. Neural networks learn the representation of programs from data in the
form of program stack traces.
</p>

<p class="text">
At any time $t$, environment observations, $e^t \in \mathcal{E}$, and program arguments, $a^t \in \mathcal{A}$, are encoded into a state vector, $s^t = f_{enc}
(e^t, a^t)$, by a domain specific encoder, $f_{enc}: \mathcal{E} \times \mathcal{A} \to \Re^D$. The state vector, $s^t$, and the current program embedding $p^t
\in\Re^P$ are fed through a Long Short Term Memory Network, $h^t = f_{lstm}(s^t, p^t, h^{t - 1})$, to determine when to return from the present program, $r^t =
f_{end}(h^t)$, when to call another subprogram, $k^t = f_{prog}(h^t)$, and which arguments to pass to the next program, $a^{t + 1} = f_{arg}(h^t)$. $f_{end}:
\Re^M \to [0, 1]$, returns the probability of ending the program, $f_{prog}: \Re^M \to \Re^K$ returns a key used to index the next subprogram in a memory
$M^{key} \in \Re^{N \times K}$, while $f_{arg}: \Re^M \to \mathcal{A}$ generates arguments. From a memory $M^{prog} \in \Re^{N \times P}$, the next program
embedding is predicted as $p^{t + 1} = M^{prog}_{i}$ where $i = \underset{i \in \mathbb{N}_{\leq N}}{\text{argmax}} (M^{key}_i)^T \cdot k^t$. The next
environment is generated by a domain specific transition map, $e^{t + 1} = f_{env}(e^t, p^t, a^t)$. Training attempts to maximize the log likelihood of the
desired outputs, $\log P(\xi_{out} | \xi_{in}) = \displaystyle\sum^T_{t = 1} \log P(\xi^t_{out} | \xi^1_{inp} ... \xi^t_{inp})$ where $P(\xi^t_{out} |
\xi^1_{inp} ... \xi^t_{inp}) = P(i^{t + 1} | h^t) \cdot P(a^{t + 1} | h^t) \cdot P(r^t | h^t)$.
</p>
