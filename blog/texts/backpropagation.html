<p class="text">
As far as I know, gradient descent is an abstract mathematical concept. And you cannot simply ask a computer to understand and apply an abstract mathematical
concept - even humans have trouble, sometimes - at least, not yet. Explicitly coding a means to calculate the derivatives for each of the parameters is far too
tedious to even consider, because there tends to be several million, if not billion of them. How then, is it done? The answer lies in backpropagation.
</p>

<p class="text">
Deconstructing the structure of neural networks, we would find a collection of layers. These layers are mostly independent of each other. The sequential nature
of the arrangement between layers, with the outputs of one being fed into the inputs of the next, is the only link between them. The forward pass applies the
map of each of the layers sequentially to the input. It would be convenient to define a similar backward pass for gradient descent. The idea is to feed the
gradients backwards through the layers, in the reverse order, to propagate the derivative of the cost funciton, and calculate the derivatives with respect to
the parameters in each of the layers from it. How would such an idea be realized?
</p>

<p class="text">
Deconstructing the functional expression of neural networks, we would find the mapping, $f_\theta$, of the $n$-layer network collectively to simply be the
composition, $f_{\theta_n}^{(n)} \circ f_{\theta_{n - 1}}^{(n - 1)} \dots f_{\theta_2}^{(2)} \circ f_{\theta_1}^{(1)}$, of the mappings of the individual
layers, $f_{\theta_i}^{(i)}$. The output is fed into a cost function, $|f_\theta|$. Subsequently, all its gradients, $\frac{\partial |f_\theta|}{\partial
\theta_i}$, are calculated, and all the parameters, $\theta_i$, updated.
</p>

<p class="text">
Backpropagation is in essence an intelligent application of the chain rule for partial derivatives. For a map from a system of $p$ independent variables $x_n
\in \Re^p$ to another system of $q$ independent variables $x_{n + 1} \in \Re^q$, the Jacobian matrix $\frac{\partial (x_{n + 1}^{(1)} \dots x_{n + 1}^{(q)})}
{\partial (x_n^{(1)} \dots x_n^{(p)})}$ is defined as $\mathbb{J}_{i, j} = \frac{\partial x_{n + 1}^{(i)}}{\partial x_n^{(j)}}$. To convert the partial
derivatives of a function, $f$, with respect to the system $x_{n + 1}$ into the those with respect to the system $x_n$, we use the Jacobian as $\frac{\partial
f}{\partial x_n} = \frac{\partial (x_{n + 1}^{(1)} \dots x_{n + 1}^{(q)})}{\partial (x_n^{(1)} \dots x_n^{(p)})}^T \cdot \frac{\partial f}{\partial x_{n + 1}}$.
</p>

<p class="text">
Each layer, $l_i$, in a neural network defines a new system of independent variables, $x_{i + 1}$ as its output. For an $n$-layer network as a whole, $x_1$ is
its input and $x_{n + 1}$ is its output. This definition implies that $x_{i + 1}$ is a function of $x_i$ and $\theta_i$, and them alone. If $l_i$ feeds forward
$\overrightarrow{f_{\theta_i}^{(i)}}: \begin{cases} & \Re^p \to \Re^q \\ & x_i \mapsto x_{i + 1} \end{cases}$, it backpropagates $\overleftarrow{f_{\theta_i}
^{(i)}}: \begin{cases} & \Re^q \to \Re^p \\ & \frac{\partial |f_\theta|}{\partial x_{i + 1}} \mapsto \frac{\partial |f_\theta|}{\partial x_i}\end{cases}$.
Hence, $l_i$, provided $\frac{\partial |f_\theta|}{\partial x_{i + 1}}$ by backpropagation, can calculate both $\frac{\partial |f_\theta|}{\partial x_i}$,
necessary for further backpropagation, and $\frac{\partial |f_\theta|}{\partial \theta_i}$, necessary for parameter updates, by the chain rule.
</p>
